{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up environement\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/share/spark-3.1.1-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing All required packages\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,ArrayType,DateType,FloatType,TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining sparksession\n",
    "spark = SparkSession.builder.master('local[2]').appName('SparkProject_In').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying File Schemas (i.e. Mentioning column name, datatype and Null value status of each column):-\n",
    "\n",
    "#1 Aisles Schema:-\n",
    "aisles_schema= StructType([StructField('aisle_id',IntegerType(),False),StructField('aisle',StringType(),True)])\n",
    "\n",
    "#2 Departments_schema:-\n",
    "department_schema=StructType([StructField('department_id',IntegerType(),False),StructField('department',StringType(),True)])\n",
    "#3 order_schema:-\n",
    "orders_schema=StructType([StructField('order_id',IntegerType(),False),StructField('user_id',IntegerType(),True),StructField('eval_set',StringType(),True),StructField('order_number',IntegerType(),True),StructField('order_dow',IntegerType(),True),StructField('order_hour_of_day',IntegerType(),True),StructField('days_since_prior_order',IntegerType(),True)])\n",
    "\n",
    "#4 prior_order_schema and train_order_schema:-\n",
    "prior_order_schema=StructType([StructField('order_id',IntegerType(),True),StructField('product_id',IntegerType(),True),StructField('add_to_cart_order',IntegerType(),True),StructField('reordered',IntegerType(),True)])\n",
    "#5 Products_schema:-\n",
    "products_schema=StructType([StructField('product_id',IntegerType(),False),StructField('product_name',StringType(),True),StructField('aisle_id',StringType(),True),StructField('department_id',StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Extracting Data:-\n",
    "#defining file path from where to read the files and output path\n",
    "#Note:- The data has been copied to local and then given the local path here as I was facing issues with Insofe cluster for my IP.\n",
    "dataset_path='/home/fai10086/'\n",
    "output_path=dataset_path+\"/output/\"\n",
    "\n",
    "#reading files as dataframes:-\n",
    "\n",
    "#aisles\n",
    "aisles_df = spark.read.schema(aisles_schema).option(\"delimeter\",\",\").option(\"header\",\"True\").csv('/home/fai10086/Data_sets/aisles.csv')\n",
    "\n",
    "#departments:-\n",
    "department_df = spark.read.schema(department_schema).option(\"header\",\"True\").csv('/home/fai10086/Data_sets/departments.csv')\n",
    "#orders:-\n",
    "orders_df = spark.read.schema(orders_schema).option(\"header\",\"True\").csv('/home/fai10086/Data_sets/orders.csv')\n",
    "\n",
    "#prior_order:-\n",
    "prior_order_df = spark.read.schema(prior_order_schema).option(\"header\",\"True\").csv('/home/fai10086/Data_sets/prior_order.csv')\n",
    "\n",
    "#products:- reading products file as rdd as it has some noises later on it has been converted to data frame after removing noises. \n",
    "#All other files have been read as csv\n",
    "products_rdd = spark.sparkContext.textFile('/home/fai10086/Data_sets/products.csv')\n",
    "\n",
    "#train_order:-\n",
    "train_order_df= spark.read.schema(prior_order_schema).option(\"header\",\"True\").csv('/home/fai10086/Data_sets/train_order.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing noises from products data:- removing unwanted characters from records like:- '\\' , '\"' , ',' etc\n",
    "#after removing noises we will convert it to dataframe\n",
    "def remove_noise(row):\n",
    "    if '\"' in row:\n",
    "        first=row.index('\"')\n",
    "        last=row.index('\"',first+1)\n",
    "        part_a=row[0:first]\n",
    "        part_b=row[first:last+1].replace(\", \",\" - \").replace('\"','')\n",
    "        part_c=row[last+1:]\n",
    "        row=(part_a+part_b+part_c).replace('\\\"',\"\").split(\",\")\n",
    "        return [int(row[0]),row[1],row[2],row[3]]\n",
    "    else:\n",
    "        row = row.replace('\\\"',\"\").split(\",\")\n",
    "        return [int(row[0]),row[1],row[2],row[3]]\n",
    "\n",
    "header=products_rdd.first()\n",
    "products_rdd_mo=products_rdd.filter(lambda x : x!=header).map(lambda x : remove_noise(x))\n",
    "products_df=products_rdd_mo.toDF(products_schema) # product dataframe creation from product rdd after removing noises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing columns of all data frames:\n",
    "print('Showing columns of all data frames:')\n",
    "print('\\naisles :\\n',aisles_df.columns)\n",
    "print('\\nproducts :\\n',products_df.columns)\n",
    "print('\\ndepartments :\\n',department_df.columns)\n",
    "print('\\norders :\\n',orders_df.columns)\n",
    "print('\\nprior order :\\n',prior_order_df.columns)\n",
    "print('\\ntrain order :\\n',train_order_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing Data types of all data frames:\n",
    "\n",
    "print('\\nShowing Data types of all data frames:')\n",
    "print('Aisles')\n",
    "aisles_df.printSchema()\n",
    "print('Department')\n",
    "department_df.printSchema()\n",
    "print('Products')\n",
    "products_df.printSchema()\n",
    "print('Orders')\n",
    "orders_df.printSchema()\n",
    "print('Prior Order')\n",
    "prior_order_df.printSchema()\n",
    "print('Train Order')\n",
    "train_order_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B) Transformation:- Data Processing Part\n",
    "\n",
    "#Creating Tables from dataframes for aggregation purposes:-\n",
    "aisles_df.createOrReplaceTempView('aisles') # aisles table\n",
    "department_df.createOrReplaceTempView('department') # department table\n",
    "orders_df.createOrReplaceTempView('orders') # orders table\n",
    "prior_order_df.createOrReplaceTempView('prior_order') #prior_order table\n",
    "products_df.createOrReplaceTempView('products') #products table\n",
    "train_order_df.createOrReplaceTempView('train_order') #train_order table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the records from tables\n",
    "print('\\nDisplaying records from tables')\n",
    "spark.sql(\"select * from aisles\").show(2)\n",
    "spark.sql(\"select * from department\").show(2)\n",
    "spark.sql(\"select * from orders\").show(2)\n",
    "spark.sql(\"select * from prior_order\").show(2)\n",
    "spark.sql(\"select * from products\").show(2)\n",
    "spark.sql(\"select * from train_order\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregating products, prior_order and train_order data first (just to make the process easy abd simple)\n",
    "aggregated_table_part_1 =spark.sql('''SELECT p.product_id, product_name, aisle_id, department_id, order_id, add_to_cart_order, reordered\n",
    "                                      FROM products p INNER JOIN train_order to ON to.product_id=p.product_id\n",
    "                                      UNION ALL\n",
    "                                      SELECT p.product_id, product_name, aisle_id, department_id, order_id,add_to_cart_order,reordered\n",
    "                                      FROM products p INNER JOIN prior_order po ON po.product_id=p.product_id''')\n",
    "\n",
    "#creating table from aggregated_table_part_1 dataframe for further aggregation\n",
    "aggregated_table_part_1.createOrReplaceTempView(\"Combined_table\")\n",
    "\n",
    "#aggregating all tables as per the data model\n",
    "fully_combined_table = spark.sql('''SELECT product_id, product_name, t.aisle_id,aisle, d.department_id, department, o.order_id, user_id, \n",
    "                                    add_to_cart_order, reordered,eval_set, order_number, order_dow, order_hour_of_day, days_since_prior_order\n",
    "                                   FROM Combined_table t \n",
    "                                   INNER JOIN orders o ON o.order_id=t.order_id \n",
    "                                   INNER JOIN aisles a ON a.aisle_id=t.aisle_id\n",
    "                                   INNER JOIN department d ON d.department_id=t.department_id''')\n",
    "\n",
    "                \n",
    "#C Loading results to destination:- writing back tranformed data to destination (data lake):-\n",
    "\n",
    "fully_combined_table.coalesce(1).write.option(\"header\",True).csv(output_path)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Null Values\n",
    "print('Null Value count:')\n",
    "{col : aisles_df.filter(aisles_df[col].isNull()).count() for col in aisles_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{col : department_df.filter(department_df[col].isNull()).count() for col in department_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{col : products_df.filter(products_df[col].isNull()).count() for col in products_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{col : orders_df.filter(orders_df[col].isNull()).count() for col in orders_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{col : prior_order_df.filter(prior_order_df[col].isNull()).count() for col in prior_order_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{col : train_order_df.filter(train_order_df[col].isNull()).count() for col in train_order_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
